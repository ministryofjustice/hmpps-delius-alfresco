version: "3"

vars:
  ENV: "{{.ENV}}"
  FROM: "{{.FROM}}"
  TO: "{{.TO}}"
  FORCE: "{{.FORCE}}"
  NAMESPACE:
    sh: |
      if [ -z "{{.ENV}}" ]; then
        echo "ERROR: ENV variable is not set" >&2
        exit 1
      elif [ "{{.ENV}}" = "poc" ]; then
        echo "hmpps-delius-alfrsco-{{.ENV}}"
      else
        echo "hmpps-delius-alfresco-{{.ENV}}"
      fi
  BUCKET_NAME:
    sh: kubectl get secret s3-bucket-output -n {{.NAMESPACE}} -o jsonpath='{.data.BUCKET_NAME}' | base64 --decode
  OPEN_SEARCH_PREFIX:
    sh: kubectl get svc --namespace {{.NAMESPACE}} | grep 'opensearch-proxy-service-cloud-platform' | awk '{ print $1 }'
  OPENSEARCH_HOST:
    sh: echo "{{.OPEN_SEARCH_PREFIX}}.{{.NAMESPACE}}.svc.cluster.local"
  MESSAGEBROKER_URL:
    sh: |
      kubectl get secrets amazon-mq-broker-secret -o json -n {{.NAMESPACE}} | \
      jq -r ".data | map_values(@base64d) | .BROKER_URL" | \
      sed -e 's/(/\\(/g' -e 's/)/\\)/g' -e 's/,/\\,/g'
  ALLOWLIST:
    sh: yq 'join(",")' ./kustomize/{{.ENV}}/allowlist.yaml
  RDS_JDBC_URL:
    sh: kubectl get secrets rds-instance-output --namespace {{.NAMESPACE}} -o json | jq -r ".data | map_values(@base64d) | .RDS_JDBC_URL"
  DEBUG: "false"
  DEBUG_FLAG:
    sh: if [ "{{.DEBUG}}" = "true" ]; then echo "--debug"; else echo ""; fi
  HELM_POST_RENDERER_ARGS:
    sh: if [ "{{.DEBUG}}" = "true" ]; then echo "-d true"; else echo "-d false"; fi
  ATOMIC: "true"
  ATOMIC_FLAG:
    sh: if [ "{{.ATOMIC}}" = "true" ]; then echo "--atomic"; else echo ""; fi
  DRY_RUN: "false"
  DRY_RUN_FLAG:
    sh: if [ "{{.DRY_RUN}}" = "true" ]; then echo "--dry-run"; else echo ""; fi
  CHART_VERSION: "7.0.3"
  RELEASE_NAME: "alfresco-content-services"
  MAX_REPO_REPLICAS:
    sh: yq '.spec.maxReplicas' ./kustomize/{{.ENV}}/hpa-repository.yaml
  MIN_REPO_REPLICAS:
    sh: yq '.spec.minReplicas' ./kustomize/{{.ENV}}/hpa-repository.yaml
  MAX_TIKA_REPLICAS:
    sh: yq '.spec.maxReplicas' ./kustomize/{{.ENV}}/hpa-tika.yaml
  MIN_TIKA_REPLICAS:
    sh: yq '.spec.minReplicas' ./kustomize/{{.ENV}}/hpa-tika.yaml


tasks:
  # Perform a helm upgrade on the alfresco-content-services chart
  helm_upgrade:
    cmds:
      - echo "NAMESPACE set to {{.NAMESPACE}}"
      - echo "BUCKET_NAME set to {{.BUCKET_NAME}}"
      - echo "OPEN_SEARCH_PREFIX set to {{.OPEN_SEARCH_PREFIX}}"
      - echo "OPENSEARCH_HOST set to {{.OPENSEARCH_HOST}}"
      - echo "RDS_JDBC_URL set to {{.RDS_JDBC_URL}}"
      - echo "DEBUG set to {{.DEBUG}}"
      - echo "DRY_RUN set to {{.DRY_RUN}}"
      - task: helm_repo_add
      - task: update_allowlist
      - task: helm_upgrade_install
        vars:
          NAMESPACE: "{{.NAMESPACE}}"
          BUCKET_NAME: "{{.BUCKET_NAME}}"
          OPENSEARCH_HOST: "{{.OPENSEARCH_HOST}}"
          RDS_JDBC_URL: "{{.RDS_JDBC_URL}}"
          DEBUG_FLAG: "{{.DEBUG_FLAG}}"
          ATOMIC_FLAG: "{{.ATOMIC_FLAG}}"
          DRY_RUN_FLAG: "{{.DRY_RUN_FLAG}}"
      - task: reset_allowlist
    silent: true

  prepare_namespace:
    internal: true
    cmds:
      - |
        export BUCKET_NAME=$(kubectl get secrets s3-bucket-output -n {{.NAMESPACE}} -o jsonpath='{.data.BUCKET_NAME}' | base64 -d)
        if [ "${ENV}" = "poc" ]; then
          export NAMESPACE=hmpps-delius-alfrsco-${ENV}
        else
          export NAMESPACE=hmpps-delius-alfresco-${ENV}
        fi
        export OPENSEARCH_PREFIX=$(kubectl get svc --namespace ${NAMESPACE} | grep 'opensearch-proxy-service-cloud-platform' | awk '{ print $1 }')
        export OPENSEARCH_HOST=${OPENSEARCH_PREFIX}.${NAMESPACE}.svc.cluster.local
        export RDS_JDBC_URL=$(kubectl get secrets rds-instance-output --namespace ${NAMESPACE} -o json | jq -r ".data | map_values(@base64d) | .RDS_JDBC_URL")
        export EXTRACTED=$(yq 'join(",")' ./kustomize/${ENV}/allowlist.yaml)
        echo "Using namespace: ${NAMESPACE}"

  update_allowlist:
    internal: true
    dir: ./kustomize/{{.ENV}}
    cmds:
      - |
        export ALLOWLIST={{.ALLOWLIST}}
        yq '.metadata.annotations."nginx.ingress.kubernetes.io/whitelist-source-range" = strenv(ALLOWLIST)' -i ./patch-ingress-repository.yaml
        yq '.metadata.annotations."nginx.ingress.kubernetes.io/whitelist-source-range" = strenv(ALLOWLIST)' -i ./patch-ingress-share.yaml

  helm_repo_add:
    internal: true
    cmds:
      - helm repo add alfresco https://kubernetes-charts.alfresco.com/stable --force-update

  helm_upgrade_install:
    internal: true
    dir: ./kustomize/{{.ENV}}
    cmds:
      - |
        RELEASE_NAME="{{.RELEASE_NAME}}"
        helm upgrade --install $RELEASE_NAME alfresco/$RELEASE_NAME --version {{.CHART_VERSION}} --namespace {{.NAMESPACE}} \
        --values=../base/values.yaml --values=../base/values-versions.yaml --values=values.yaml \
        --set s3connector.config.bucketName={{.BUCKET_NAME}} \
        --set database.url={{.RDS_JDBC_URL}} \
        --set-string messageBroker.url="{{.MESSAGEBROKER_URL}}" \
        --set global.search.url=http://{{.OPENSEARCH_HOST}}:8080 \
        --set global.search.host={{.OPENSEARCH_HOST}} \
        --wait --timeout=60m \
        --post-renderer ../kustomizer.sh --post-renderer-args "{{.HELM_POST_RENDERER_ARGS}}" \
        {{.DEBUG_FLAG}} {{.ATOMIC_FLAG}} {{.DRY_RUN_FLAG}}
        echo " "
        echo "***** Helm upgrade completed *****"
        echo "Helm revision: $(helm list -n {{.NAMESPACE}} | grep $RELEASE_NAME | awk '{print $10}')"
        echo "Chart version: $(helm list -n {{.NAMESPACE}} | grep $RELEASE_NAME | awk '{print $9}')"
        echo "ACS Version: $(helm list -n {{.NAMESPACE}} | grep $RELEASE_NAME | awk '{print $10}')"
        echo " "

  reset_allowlist:
    internal: true
    dir: ./kustomize/{{.ENV}}
    cmds:
      - yq '.metadata.annotations."nginx.ingress.kubernetes.io/whitelist-source-range" = "placeholder"' -i patch-ingress-repository.yaml
      - yq '.metadata.annotations."nginx.ingress.kubernetes.io/whitelist-source-range" = "placeholder"' -i patch-ingress-share.yaml

  # reindexes by id from the FROM_ID to the TO_ID
  # ID FORMAT: alf-node id
  reindex_by_id:
    desc: >
      Reindex Alfresco documents and/or metadata by ID range using Helm.
      Requires FROM and TO environment variables to define the ID range.

      Automatically scales up deployments if the ID range is greater than 100,000.

      Optionally set INDEX_CONTENT to 'false' to skip content indexing and only reindex metadata.
      Defaults to 'true'.

      Example usage:
        FROM=100000 TO=200000 task reindex_by_id
        FROM=100000 TO=200000 INDEX_CONTENT=false task reindex_by_id
    cmds:
      - |
        if [ -z "{{.FROM}}" ]; then
        echo "ERROR: FROM variable is not set" >&2
          exit 1
        fi
        if [ -z "{{.TO}}" ]; then
          echo "ERROR: TO variable is not set" >&2
          exit 1
        fi

        INDEX_CONTENT="{{.INDEX_CONTENT}}"
        if [ -z "$INDEX_CONTENT" ]; then
          INDEX_CONTENT="true"
        fi

        RANGE=$((TO - FROM))
        echo "üìä ID Range: $FROM to $TO (count: $RANGE)"

        if [ "$RANGE" -gt 100000 ]; then
          echo "üöÄ Large ID range detected. Scaling up repository and tika deployments..."
          task reindex_scale_up ENV={{.ENV}}
        else
          echo "‚ÑπÔ∏è Small ID range detected. No scaling needed."
        fi

        task delete_existing_reindex_configmaps_for_range ENV={{.ENV}} FROM={{.FROM}} TO={{.TO}}

        helm install "reindex-default-$(openssl rand -hex 4)" ./jobs/reindex --set "global.elasticsearch.host={{.OPENSEARCH_HOST}}" --set "fromId={{.FROM}}" --set "toId={{.TO}}" --set "content=${INDEX_CONTENT}" --namespace {{.NAMESPACE}}

  # reindexes by date from the FROM_DATE to the TO_DATE
  # DATE FORMAT: YYYYMMDDHHMM
  # To only reindex metadata, set INDEX_CONTENT to false e.g.
  # INDEX_CONTENT=false task reindex_by_date
  reindex_by_date:
    desc: >
      Reindex Alfresco documents and/or metadata by date range using Helm.
      Requires FROM and TO environment variables to define the range.

      Optionally set INDEX_CONTENT to 'false' to skip content indexing and only reindex metadata.
      Defaults to 'true' (index both content and metadata).

      Example usage:
        FROM=201706072250 TO=201706072350 task reindex_by_date
        FROM=201706072250 TO=201706072350 INDEX_CONTENT=false task reindex_by_date    
    cmds:
      - |
        if [ -z "{{.FROM}}" ]; then
          echo "ERROR: FROM variable is not set" >&2
          exit 1
        fi
        if [ -z "{{.TO}}" ]; then
          echo "ERROR: TO variable is not set" >&2
          exit 1
        fi

        INDEX_CONTENT="{{.INDEX_CONTENT}}"
        if [ -z "$INDEX_CONTENT" ]; then
          INDEX_CONTENT="true"
        fi

        FROM="{{.FROM}}"
        TO="{{.TO}}"
        NS="{{.NAMESPACE}}"

        CM="reindexing-${FROM}-${TO}-configmap"
        PREFIX_CM="reindexing-${FROM}-${TO}-prefixes-configmap"

        echo "üîç Looking for active pods starting with reindexing-* in namespace $NS..."
        REINDEX_PODS=$(kubectl get pods -n "$NS" --no-headers | awk '/^reindexing-/ && $3 != "Completed" && $3 != "Failed" { print $1 }')

        CM_IN_USE=false
        for POD in $(echo "$ACTIVE_PODS" | jq -r '.items[].metadata.name'); do
          POD_JSON=$(kubectl get pod "$POD" -n "$NS" -o json)

          if echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.volumes[]?.configMap.name == $CM' >/dev/null ||
            echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.containers[].envFrom[]?.configMapRef.name == $CM' >/dev/null ||
            echo "$POD_JSON" | jq -e --arg CM "$PREFIX_CM" '.spec.volumes[]?.configMap.name == $CM' >/dev/null ||
            echo "$POD_JSON" | jq -e --arg CM "$PREFIX_CM" '.spec.containers[].envFrom[]?.configMapRef.name == $CM' >/dev/null; then
            echo "‚ö†Ô∏è  ConfigMap $CM or $PREFIX_CM is in use by pod $POD. Skipping deletion."
            CM_IN_USE=true
          fi
        done

        if [ "$CM_IN_USE" = false ]; then
          echo "‚úÖ No active pods are using the ConfigMaps. Safe to delete."
          kubectl delete configmap "$CM" -n "$NS" --ignore-not-found
          kubectl delete configmap "$PREFIX_CM" -n "$NS" --ignore-not-found
        else
          echo "‚ö†Ô∏è  Skipping ConfigMap deletion to avoid disrupting running pods."
        fi

        echo "Scaling up deployments before reindexing..."
        task reindex_scale_up ENV={{.ENV}}

        echo "Running Helm install to run job..."
        helm install "reindex-default-$(openssl rand -hex 4)" ./jobs/reindex_date --set "global.elasticsearch.host={{.OPENSEARCH_HOST}}" --set "fromTime=${FROM}" --set "toTime=${TO}" --set "content=${INDEX_CONTENT}" --namespace ${NS}

  delete_existing_reindex_configmaps_for_range:
    desc: "Delete existing reindexing configmaps for a specific FROM-TO ID range"
    cmds:
      - |
        FROM="{{.FROM}}"
        TO="{{.TO}}"
        NS="{{.NAMESPACE}}"

        if [ -z "$FROM" ] || [ -z "$TO" ]; then
          echo "‚ùå FROM and TO must be set"
          exit 1
        fi

        PREFIX="reindexing-${FROM}-${TO}-"
        echo "üîç Searching for configmaps with prefix $PREFIX in $NS..."

        CONFIGMAPS=$(kubectl get configmaps -n "$NS" -o json | jq -r '.items[].metadata.name' | grep "^${PREFIX}" || true)

        if [ -z "$CONFIGMAPS" ]; then
          echo "‚úÖ No matching ConfigMaps found."
          exit 0
        else
          echo "Found ConfigMaps: $CONFIGMAPS"
        fi

        # Get all active pods (not Completed or Failed)
        ACTIVE_PODS=$(kubectl get pods -n "$NS" --field-selector=status.phase!=Succeeded,status.phase!=Failed -o json)

        for CM in $CONFIGMAPS; do
          echo "üîé Checking if ConfigMap $CM is in use..."

          CM_IN_USE=false
          for POD in $(echo "$ACTIVE_PODS" | jq -r '.items[].metadata.name'); do
            POD_JSON=$(kubectl get pod "$POD" -n "$NS" -o json)

            if echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.volumes[]?.configMap.name == $CM' >/dev/null ||
              echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.containers[].envFrom[]?.configMapRef.name == $CM' >/dev/null; then
              echo "‚ö†Ô∏è  ConfigMap $CM is in use by pod $POD"
              CM_IN_USE=true
              break
            fi
          done

          if [ "$CM_IN_USE" = false ]; then
            echo "üóëÔ∏è  Deleting unused ConfigMap $CM"
            kubectl delete configmap "$CM" -n "$NS" --ignore-not-found
          else
            echo "‚è≠Ô∏è  Skipping $CM as it's currently in use"
          fi
        done

  helm_uninstall_prefix:
    vars:
      PREFIX: "{{.PREFIX}}"
    cmds:
      - |
        helm list -n {{.NAMESPACE}} -q | grep "^{{.PREFIX}}" | while IFS= read -r release; do
          echo "Uninstalling release: $release"
          helm uninstall "$release" -n {{.NAMESPACE}}
        done

  kubectl_remove_pods_prefix:
    vars:
      PREFIX: "{{.PREFIX}}"
      FORCE_FLAG:
        sh: if [ "{{.FORCE}}" = "true" ]; then echo "--force"; else echo ""; fi
    cmds:
      - |
        kubectl get pods -n {{.NAMESPACE}} | grep "^{{.PREFIX}}" | awk '{print $1}' | while IFS= read -r pod; do
          echo "Deleting pod: $pod"
          kubectl delete pod "$pod" -n {{.NAMESPACE}} {{.FORCE_FLAG}}
        done

  cleanup_unused_reindex_configmaps:
    desc: "Delete all unused reindexing-* configmaps in a given namespace"
    cmds:
      - |
        NS="{{.NAMESPACE}}"

        echo "üîç Finding all reindexing ConfigMaps in namespace $NS..."
        CONFIGMAPS=$(kubectl get configmaps -n "$NS" -o json | jq -r '.items[].metadata.name' | grep '^reindexing-' | wc -l)

        if [ "$CONFIGMAPS" -eq 0 ]; then
          echo "‚úÖ No reindexing ConfigMaps found."
          exit 0
        else
          CONFIGMAPS=$(kubectl get configmaps -n "$NS" -o json | jq -r '.items[].metadata.name' | grep '^reindexing-')
        fi

        # Get all active pods (not Completed/Failed)
        ACTIVE_PODS=$(kubectl get pods -n "$NS" --field-selector=status.phase!=Succeeded,status.phase!=Failed -o json)

        for CM in $CONFIGMAPS; do
          echo "üîé Checking if ConfigMap $CM is in use..."

          CM_IN_USE=false
          for POD in $(echo "$ACTIVE_PODS" | jq -r '.items[].metadata.name'); do
            POD_JSON=$(kubectl get pod "$POD" -n "$NS" -o json)

            if echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.volumes[]?.configMap.name == $CM' >/dev/null ||
              echo "$POD_JSON" | jq -e --arg CM "$CM" '.spec.containers[].envFrom[]?.configMapRef.name == $CM' >/dev/null; then
              echo "‚ö†Ô∏è  In use by pod $POD"
              CM_IN_USE=true
              break
            fi
          done

          if [ "$CM_IN_USE" = false ]; then
            echo "üóëÔ∏è  Deleting unused ConfigMap $CM"
            kubectl delete configmap "$CM" -n "$NS"
          else
            echo "‚è≠Ô∏è  Skipping $CM as it's in use"
          fi
        done

  restart_deployments:
    desc: "Restart all ACS deployments in the specified namespace"
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        RELEASE_NAME="{{.RELEASE_NAME}}"
        echo "Restarting deployments in namespace $NS"
        # Get all deployment names, then filter only the ones that start with the release name
        DEPLOYMENTS=$(kubectl get deployments -n "$NS" -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep "^${RELEASE_NAME}-")
        if [ -z "$DEPLOYMENTS" ]; then
          echo "No deployments found in namespace $NS"
          exit 0
        fi

        for DEPLOYMENT in $DEPLOYMENTS; do
          echo "Restarting deployment: $DEPLOYMENT"
          kubectl rollout restart deployment "$DEPLOYMENT" -n "$NS"
        done

  reindex_scale_up:
    desc: "Scale up ACS deployments to their maxReplicas when reindexing by setting HPA minReplicas to match maxReplicas"
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        RELEASE_NAME="{{.RELEASE_NAME}}"
        MAX_REPO_REPLICAS="{{.MAX_REPO_REPLICAS}}"
        MIN_REPO_REPLICAS="{{.MIN_REPO_REPLICAS}}"
        MAX_TIKA_REPLICAS="{{.MAX_TIKA_REPLICAS}}"
        MIN_TIKA_REPLICAS="{{.MIN_TIKA_REPLICAS}}"

        echo "Updating HPA minReplicas to match maxReplicas for Repository"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MAX_REPO_REPLICAS}}}"

        echo "Updating HPA minReplicas to match maxReplicas for Tika"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MAX_TIKA_REPLICAS}}}"

  reindex_scale_down:
    desc: "Scale down ACS deployments to their default minReplicas after reindexing by resetting HPA minReplicas"
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        RELEASE_NAME="{{.RELEASE_NAME}}"
        MAX_REPO_REPLICAS="{{.MAX_REPO_REPLICAS}}"
        MIN_REPO_REPLICAS="{{.MIN_REPO_REPLICAS}}"
        MAX_TIKA_REPLICAS="{{.MAX_TIKA_REPLICAS}}"
        MIN_TIKA_REPLICAS="{{.MIN_TIKA_REPLICAS}}"

        echo "Updating HPA minReplicas to original minReplicas value for Repository"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MIN_REPO_REPLICAS}}}"

        echo "Updating HPA minReplicas to original minReplicas value for Tika"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MIN_TIKA_REPLICAS}}}"

  scale_up_deployments:
    desc: "Scale up all ACS deployments based on config file defaults"
    dir: ./kustomize/{{.ENV}}
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        RELEASE_NAME="{{.RELEASE_NAME}}"

        # HPA configured deployments
        MAX_REPO_REPLICAS="{{.MAX_REPO_REPLICAS}}"
        MIN_REPO_REPLICAS="{{.MIN_REPO_REPLICAS}}"
        MAX_TIKA_REPLICAS="{{.MAX_TIKA_REPLICAS}}"
        MIN_TIKA_REPLICAS="{{.MIN_TIKA_REPLICAS}}"

        echo "Reset HPA Replicas for Repository"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MIN_REPO_REPLICAS}}}"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"maxReplicas\": ${MAX_REPO_REPLICAS}}}"

        echo "Reset HPA Replicas for Tika"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": ${MIN_TIKA_REPLICAS}}}"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"maxReplicas\": ${MAX_TIKA_REPLICAS}}}"

        kubectl scale deployment $RELEASE_NAME-alfresco-repository --replicas=$MIN_REPO_REPLICAS
        kubectl scale deployment $RELEASE_NAME-tika --replicas=$MIN_TIKA_REPLICAS

        # non-HPA configured deployments
        # Load default replica counts from values.yaml or base values.yaml
        TRANSFORM_ROUTER_REPLICAS=$(yq '(.["alfresco-transform-service"].transformrouter.replicaCount) // (load("../base/values.yaml") | .["alfresco-transform-service"].transformrouter.replicaCount) // 2' values.yaml)

        # The below deployments use the same transformrouter default replica count if not specified
        BASE_TRANSFORM_ROUTER_REPLICAS=$(yq '(["alfresco-transform-service"].transformrouter.replicaCount) // 2' ../base/values.yaml)

        TRANSFORM_MISC_REPLICAS=$(yq '.["alfresco-transform-service"].transformmisc.replicaCount // ""' values.yaml); [ -z "$TRANSFORM_MISC_REPLICAS" ] && TRANSFORM_MISC_REPLICAS=$(yq '.["alfresco-transform-service"].transformmisc.replicaCount // ""' ../base/values.yaml); [ -z "$TRANSFORM_MISC_REPLICAS" ] && TRANSFORM_MISC_REPLICAS=$BASE_TRANSFORM_ROUTER_REPLICAS

        IMAGEMAGICK_REPLICAS=$(yq '.["alfresco-transform-service"].imagemagick.replicaCount // ""' values.yaml); [ -z "$IMAGEMAGICK_REPLICAS" ] && IMAGEMAGICK_REPLICAS=$(yq '.["alfresco-transform-service"].imagemagick.replicaCount // ""' ../base/values.yaml); [ -z "$IMAGEMAGICK_REPLICAS" ] && IMAGEMAGICK_REPLICAS=$BASE_TRANSFORM_ROUTER_REPLICAS

        LIBREOFFICE_REPLICAS=$(yq '.["alfresco-transform-service"].libreoffice.replicaCount // ""' values.yaml); [ -z "$LIBREOFFICE_REPLICAS" ] && LIBREOFFICE_REPLICAS=$(yq '.["alfresco-transform-service"].libreoffice.replicaCount // ""' ../base/values.yaml); [ -z "$LIBREOFFICE_REPLICAS" ] && LIBREOFFICE_REPLICAS=$BASE_TRANSFORM_ROUTER_REPLICAS

        PDFRENDERER_REPLICAS=$(yq '.["alfresco-transform-service"].pdfrenderer.replicaCount // ""' values.yaml); [ -z "$PDFRENDERER_REPLICAS" ] && PDFRENDERER_REPLICAS=$(yq '.["alfresco-transform-service"].pdfrenderer.replicaCount // ""' ../base/values.yaml); [ -z "$PDFRENDERER_REPLICAS" ] && PDFRENDERER_REPLICAS=$BASE_TRANSFORM_ROUTER_REPLICAS

        PATH_REPLICAS=$(yq '(.["alfresco-transform-service"].path.replicaCount) // (load("../base/values.yaml") | .["alfresco-search-enterprise"].liveIndexing.path.replicaCount) // 1' values.yaml)
        CONTENT_REPLICAS=$(yq '(.["alfresco-search-enterprise"].liveIndexing.content.replicaCount) // (load("../base/values.yaml") | .["alfresco-search-enterprise"].liveIndexing.content.replicaCount) // 1' values.yaml)
        METADATA_REPLICAS=$(yq '(.["alfresco-search-enterprise"].liveIndexing.metadata.replicaCount) // (load("../base/values.yaml") | .["alfresco-search-enterprise"].liveIndexing.metadata.replicaCount) // 1' values.yaml)
        MEDIATION_REPLICAS=$(yq '(.["alfresco-search-enterprise"].liveIndexing.mediation.replicaCount) // (load("../base/values.yaml") | .["alfresco-search-enterprise"].liveIndexing.mediation.replicaCount) // 1' values.yaml)

        SHARE_REPLICAS=$(yq '(.share.replicaCount) // (load("../base/values.yaml") | .share.replicaCount) // 1' values.yaml)

        kubectl scale deployment $RELEASE_NAME-share --replicas=$SHARE_REPLICAS
        kubectl scale deployment $RELEASE_NAME-imagemagick --replicas=$IMAGEMAGICK_REPLICAS
        kubectl scale deployment $RELEASE_NAME-libreoffice --replicas=$LIBREOFFICE_REPLICAS
        kubectl scale deployment $RELEASE_NAME-pdfrenderer --replicas=$PDFRENDERER_REPLICAS
        kubectl scale deployment $RELEASE_NAME-transform-misc --replicas=$TRANSFORM_MISC_REPLICAS
        kubectl scale deployment $RELEASE_NAME-transform-router --replicas=$TRANSFORM_ROUTER_REPLICAS
        kubectl scale deployment $RELEASE_NAME-alfresco-search-enterprise-content --replicas=$CONTENT_REPLICAS
        kubectl scale deployment $RELEASE_NAME-alfresco-search-enterprise-mediation --replicas=$MEDIATION_REPLICAS
        kubectl scale deployment $RELEASE_NAME-alfresco-search-enterprise-metadata --replicas=$METADATA_REPLICAS
        kubectl scale deployment $RELEASE_NAME-alfresco-search-enterprise-path --replicas=$PATH_REPLICAS


  scale_down_deployments:
    desc: "Scale down all ACS deployments to 0 replicas"
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        RELEASE_NAME="{{.RELEASE_NAME}}"

        echo "Scaling down deployments in namespace $NS"

        # HPA configured deployments

        echo "Updating HPA Replicas for Repository"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": 0}}"
        kubectl patch hpa $RELEASE_NAME-hpa-repository -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"maxReplicas\": 0}}"

        echo "Updating HPA Replicas for Tika"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"minReplicas\": 0}}"
        kubectl patch hpa $RELEASE_NAME-hpa-tika -n "$NS" \
          --type='merge' \
          -p "{\"spec\": {\"maxReplicas\": 0}}"

        DEPLOYMENTS=$(kubectl get deployments -n "$NS" -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\n' | grep "^${RELEASE_NAME}-")
        for deployment in $DEPLOYMENTS; do
            echo "Scaling down deployment: $deployment"
            kubectl scale $deployment --replicas=0
        done

  cleanup_inactive_reindex_helm_releases:
    desc: "Delete inactive Helm releases with prefix reindex-default-* if no active pods are running"
    cmds:
      - |
        NS="{{.NAMESPACE}}"
        PREFIX="reindex-default"

        echo "üîç Finding Helm releases with prefix $PREFIX in namespace $NS..."
        RELEASES=$(helm list -n "$NS" -q | grep "^${PREFIX}" || true)

        if [ -z "$RELEASES" ]; then
          echo "‚úÖ No matching Helm releases found."
          exit 0
        fi

        for RELEASE in $RELEASES; do
          echo "‚è≥ Checking release: $RELEASE"

          PODS=$(kubectl get pods -n "$NS" -l "app.kubernetes.io/instance=$RELEASE" -o json)
          ACTIVE_POD_COUNT=$(echo "$PODS" | jq '[.items[] | select(.status.phase != "Succeeded" and .status.phase != "Failed")] | length')

          if [ "$ACTIVE_POD_COUNT" -eq 0 ]; then
            if [ "{{.DRY_RUN}}" = "true" ]; then
              echo "üß™ DRY RUN: Would uninstall $RELEASE"
            else
              echo "üóëÔ∏è  Uninstalling release: $RELEASE"
              helm uninstall "$RELEASE" -n "$NS"
            fi
          else
            echo "‚è≠Ô∏è  Skipping $RELEASE ‚Äî $ACTIVE_POD_COUNT active pod(s) still running"
          fi
        done        