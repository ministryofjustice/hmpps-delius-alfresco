---
apiVersion: v1
kind: ConfigMap
metadata:
  name: migrate-s3-script
data:
  entrypoint.sh: |-
    #!/bin/sh
    set -xe

    aws configure set default.s3.max_concurrent_requests 2000
    # aws configure set default.s3.use_accelerate_endpoint true

    aws s3 sync s3://$SRC_BUCKET/$DIR s3://$DST_BUCKET/$DIR --delete --only-show-errors --region eu-west-2

    echo sync of $DIR directory completed
{{- range .Values.dirs }}
---
apiVersion: batch/v1
kind: Job
metadata:
  name: migrate-s3-{{ . | toString | replace "/" "-" }}
spec:
  template:
    spec:
      containers:
      - name: migrate-s3
        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/webops/cloud-platform-service-pod:c5f69b4624b956248001fa7c173c89a0556a457e
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1
            memory: 1Gi
        command:
        - /bin/entrypoint.sh
        env:
        - name: SRC_BUCKET
          value: {{ $.Values.srcBucket }}
        - name: DST_BUCKET
          valueFrom:
            secretKeyRef:
              name: s3-bucket-output
              key: BUCKET_NAME
        - name: DIR
          value: {{ . | quote }}
        volumeMounts:
        - name: migrate-s3-script
          mountPath: /bin/entrypoint.sh
          readOnly: true
          subPath: entrypoint.sh
        securityContext:
          allowPrivilegeEscalation: false
          privileged: false
          readOnlyRootFilesystem: false
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
              - ALL
          seccompProfile:
            type: RuntimeDefault
      serviceAccount: hmpps-migration-{{ $.Values.environment }}
      serviceAccountName: hmpps-migration-{{ $.Values.environment }}
      restartPolicy: OnFailure
      volumes:
      - name: migrate-s3-script
        configMap:
          name: migrate-s3-script
          defaultMode: 0755
  backoffLimit: 10
{{- end }}
...
